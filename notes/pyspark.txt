//SPARK
what is spark?
    distributed computational engine
    spark > mapreduce

spark process:
    read file
    go to HDFS
        go to RDD
            filter
                count (no transfers happen until spark expecutes)
    spark execution model
        driver -> worker node
               -> worker node

//PYSPARK, PYTHON FTW
pyspark

    local 
        (python spark client)
        py4j (python for java, java sparkContext)
        python spark context

    cluster

    pickles?
        turn our pickle functions into bytes

best practices for writing pyspark
    pyspark slower than corespark
        minimize python -> jvm movements
    data frames
        use when you can
    REPLs and Notebooks

what is the shape of a pyspark job
    parse CLI args and configure spark app
        read in data
        raw data to features
        fancy maths with spark
        write out data

pyspark code strucutre
    my_pyspark_proj/
        awesome/
            __init__.py
            Featurize.py
            Model.py
        bin/
        docs/
        setup.py/
        tests/
            __init__.py
            awesome_tests.py
            resources/
                data_source_sample.csv

write testable code
    write a function for anything inside a transformation
        make it static
    sepearte feature generization or data standarization from your modeling

write serializable code
    functions and the contexts they need to execute must be sieralizable
    keep functions simple. i suggest static methods
    some things are impossiblish
        db connections (shared) _=> use mapPartitions instead

testing with sparktestingbase
    provdes a sparkcontext
    quiets py4j
